"""Stage 2 â€” ä¿¡æ¯åŠ å·¥æ¨¡å—

èŒè´£ï¼šRawItem åˆ—è¡¨ â†’ ä¸‰å±‚å»é‡ â†’ èšç±» â†’ context_block â†’ äº”ç»´æ‰“åˆ† â†’
      è§’è‰²åˆ†é… â†’ Thesis æç‚¼ â†’ è¾“å‡º episode_brief dictã€‚
"""

from __future__ import annotations

import json
import logging
import re
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any

import jieba
import numpy as np
from rapidfuzz import fuzz
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_distances

from fetcher import RawItem

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# è¾“å‡ºæ•°æ®ç»“æ„
# ---------------------------------------------------------------------------


@dataclass
class ContextBlock:
    factual_summary: list[str]  # 3 å¥äº‹å®æ‘˜è¦
    historical_background: str
    sources_ranked: list[dict[str, Any]]  # [{name, authority, link}]


@dataclass
class ScoredStory:
    """åŠ å·¥åçš„æ•…äº‹ï¼Œç”¨äºè¾“å‡º episode_briefã€‚"""

    cluster_id: int
    representative_title: str
    items: list[dict[str, Any]]  # åŸå§‹ RawItem dicts
    context: ContextBlock
    scores: dict[str, int]  # äº”ç»´å¾—åˆ†
    total_score: int
    role: str  # main / supporting / quick / skip
    role_emoji: str


# ---------------------------------------------------------------------------
# ç¬¬ä¸€å±‚å»é‡ï¼šURL ç¡¬å»é‡
# ---------------------------------------------------------------------------


def _dedup_url(items: list[RawItem]) -> list[RawItem]:
    """åŸºäº normalized_link å»é‡ã€‚"""
    seen: set[str] = set()
    out: list[RawItem] = []
    for item in items:
        key = item.normalized_link
        if key in seen:
            continue
        seen.add(key)
        out.append(item)
    logger.info("URL dedup: %d â†’ %d", len(items), len(out))
    return out


# ---------------------------------------------------------------------------
# ç¬¬äºŒå±‚å»é‡ï¼šrapidfuzz æ ‡é¢˜è½¯å»é‡
# ---------------------------------------------------------------------------


def _dedup_title_fuzzy(
    items: list[RawItem],
    *,
    threshold: int = 92,
    window_hours: int = 48,
) -> list[RawItem]:
    """token_set_ratio â‰¥ threshold ä¸”æ—¶é—´å·® â‰¤ window_hours æ—¶è§†ä¸ºé‡å¤ã€‚"""
    out: list[RawItem] = []
    for item in items:
        is_dup = False
        item_dt = datetime.fromisoformat(item.published_at)
        for kept in out:
            kept_dt = datetime.fromisoformat(kept.published_at)
            if abs((item_dt - kept_dt).total_seconds()) > window_hours * 3600:
                continue
            ratio = fuzz.token_set_ratio(item.title, kept.title)
            if ratio >= threshold:
                is_dup = True
                break
        if not is_dup:
            out.append(item)
    logger.info("Fuzzy title dedup: %d â†’ %d", len(items), len(out))
    return out


# ---------------------------------------------------------------------------
# ç¬¬ä¸‰å±‚å»é‡ï¼šjieba å…³é”®è¯é‡å 
# ---------------------------------------------------------------------------


def _extract_keywords(text: str, topk: int = 10) -> set[str]:
    """æå– jieba å…³é”®è¯é›†åˆã€‚"""
    try:
        import jieba.analyse

        keywords = jieba.analyse.extract_tags(text, topK=topk)
        return set(keywords)
    except Exception:
        words = set(jieba.cut(text))
        return {w for w in words if len(w) >= 2}


def _dedup_keyword_overlap(
    items: list[RawItem],
    *,
    overlap_threshold: float = 0.35,
    title_sim_threshold: int = 85,
) -> list[RawItem]:
    """jieba å…³é”®è¯é‡å  â‰¥ overlap_threshold ä¸”æ ‡é¢˜ç›¸ä¼¼åº¦ â‰¥ title_sim è§†ä¸ºé‡å¤ã€‚"""
    out: list[RawItem] = []
    kw_cache: list[set[str]] = []
    for item in items:
        text = f"{item.title} {item.summary}"
        kws = _extract_keywords(text)
        is_dup = False
        for i, kept in enumerate(out):
            # æ ‡é¢˜ç›¸ä¼¼åº¦é—¨æ§›
            title_ratio = fuzz.token_set_ratio(item.title, kept.title)
            if title_ratio < title_sim_threshold:
                continue
            # å…³é”®è¯é‡å 
            kept_kws = kw_cache[i]
            if not kws or not kept_kws:
                continue
            overlap = len(kws & kept_kws) / min(len(kws), len(kept_kws))
            if overlap >= overlap_threshold:
                is_dup = True
                break
        if not is_dup:
            out.append(item)
            kw_cache.append(kws)
    logger.info("Keyword overlap dedup: %d â†’ %d", len(items), len(out))
    return out


def dedup_pipeline(
    items: list[RawItem],
    *,
    rapidfuzz_threshold: int = 92,
    jieba_overlap: float = 0.35,
    title_sim: int = 85,
    window_hours: int = 48,
) -> list[RawItem]:
    """ä¸‰å±‚å»é‡ç®¡çº¿ã€‚"""
    items = _dedup_url(items)
    items = _dedup_title_fuzzy(
        items, threshold=rapidfuzz_threshold, window_hours=window_hours
    )
    items = _dedup_keyword_overlap(
        items, overlap_threshold=jieba_overlap, title_sim_threshold=title_sim
    )
    return items


# ---------------------------------------------------------------------------
# èšç±»ï¼šTF-IDF + DBSCAN
# ---------------------------------------------------------------------------


@dataclass
class Cluster:
    cluster_id: int
    items: list[RawItem]
    representative: RawItem  # æ ‡é¢˜æœ€é•¿ / å…¨æ–‡æœ€ä¸°å¯Œçš„ä»£è¡¨


def cluster_stories(
    items: list[RawItem],
    *,
    ngram_range: tuple[int, int] = (2, 4),
    eps: float = 0.35,
    min_samples: int = 2,
) -> list[Cluster]:
    """TF-IDF char n-gram + DBSCAN(cosine) èšç±»ã€‚"""
    if len(items) < 2:
        return (
            [Cluster(cluster_id=0, items=items, representative=items[0])]
            if items
            else []
        )

    texts = [f"{it.title} {it.summary}" for it in items]
    vectorizer = TfidfVectorizer(
        analyzer="char",
        ngram_range=ngram_range,
        max_features=10000,
    )
    tfidf = vectorizer.fit_transform(texts)
    dist = cosine_distances(tfidf)

    db = DBSCAN(eps=eps, min_samples=min_samples, metric="precomputed")
    labels = db.fit_predict(dist)

    clusters_map: dict[int, list[int]] = {}
    noise_id = max(labels) + 1 if len(labels) > 0 else 0
    for idx, label in enumerate(labels):
        if label == -1:
            # å™ªå£°ç‚¹å„æˆä¸€ç°‡
            clusters_map[noise_id] = [idx]
            noise_id += 1
        else:
            clusters_map.setdefault(label, []).append(idx)

    clusters: list[Cluster] = []
    for cid, indices in sorted(clusters_map.items()):
        cluster_items = [items[i] for i in indices]
        # é€‰å…¨æ–‡æœ€é•¿çš„ä½œä¸ºä»£è¡¨
        representative = max(cluster_items, key=lambda x: len(x.full_text_snippet))
        clusters.append(
            Cluster(cluster_id=cid, items=cluster_items, representative=representative)
        )

    logger.info("Clustered %d items â†’ %d clusters", len(items), len(clusters))
    return clusters


# ---------------------------------------------------------------------------
# context_block å¢è¡¥
# ---------------------------------------------------------------------------

_AUTHORITY_ORDER = {
    "official": 1,
    "research": 2,
    "news": 3,
    "product": 3,
    "analysis": 3,
    "tools": 4,
    "events": 4,
    "other": 5,
}


def _build_context(
    cluster: Cluster,
    *,
    story_memory: dict[str, Any] | None = None,
    source_authority: dict[str, int] | None = None,
) -> ContextBlock:
    """æ„å»º context_blockã€‚"""
    auth_map = source_authority or _AUTHORITY_ORDER
    rep = cluster.representative

    # 3 å¥äº‹å®æ‘˜è¦ â€” å–å‰ 3 ä¸ª item çš„ summary é¦–å¥
    summaries: list[str] = []
    for item in cluster.items[:3]:
        text = item.summary or item.full_text_snippet
        if text:
            first_sentence = re.split(r"[ã€‚.ï¼!ï¼Ÿ?\n]", text)[0].strip()
            if first_sentence:
                summaries.append(first_sentence)
    if not summaries:
        summaries = [rep.title]

    # å†å²èƒŒæ™¯ â€” ç®€å•å ä½ï¼ˆåç»­å¯ä» story_memory æ‹‰å–ï¼‰
    background = ""
    if story_memory:
        # æœç´¢ story_memory ä¸­ç›¸å…³æ¡ç›®
        for mem_key, mem_val in story_memory.items():
            if any(kw in rep.title for kw in mem_key.split()):
                background = str(mem_val)
                break

    # æ¥æºæ’åº
    sources_ranked: list[dict[str, Any]] = []
    for item in cluster.items:
        auth = auth_map.get(item.source_category, 5)
        sources_ranked.append(
            {
                "name": item.source_name,
                "authority": auth,
                "link": item.link,
            }
        )
    sources_ranked.sort(key=lambda x: x["authority"])

    return ContextBlock(
        factual_summary=summaries,
        historical_background=background,
        sources_ranked=sources_ranked,
    )


# ---------------------------------------------------------------------------
# äº”ç»´æ‰“åˆ† (PLAN Â§2.4)
# ---------------------------------------------------------------------------


def _score_cluster(cluster: Cluster) -> dict[str, int]:
    """
    äº”ç»´è¯„åˆ†ï¼Œæ¯ç»´ 1-3 åˆ†ï¼Œæ€»åˆ† 5-15ã€‚
    - impact_scope: å¤šæºæŠ¥é“ â†’ 3ï¼Œ2æº â†’ 2ï¼Œå•æº â†’ 1
    - novelty: å«æ–°äº§å“/è®ºæ–‡å…³é”®è¯ â†’ 3
    - explainability: å…¨æ–‡ä¸°å¯Œåº¦
    - listener_relevance: ä¸­æ–‡å†…å®¹ + AI å¼ºç›¸å…³ â†’ 3
    - source_richness: æ¥æºæƒå¨æ€§
    """
    items = cluster.items
    rep = cluster.representative
    text = f"{rep.title} {rep.summary} {rep.full_text_snippet}".lower()

    # impact_scope
    unique_sources = len({it.source_name for it in items})
    impact = 3 if unique_sources >= 3 else (2 if unique_sources >= 2 else 1)

    # novelty
    novel_kws = (
        "é¦–æ¬¡",
        "çªç ´",
        "åˆ›æ–°",
        "first",
        "novel",
        "breakthrough",
        "state-of-the-art",
        "æ–°å‘å¸ƒ",
        "æ–°æ¨¡å‹",
        "launch",
        "release",
        "å¼€æº",
    )
    novelty = (
        3
        if any(k in text for k in novel_kws)
        else (2 if any(k in text for k in ("æ›´æ–°", "update", "å‡çº§")) else 1)
    )

    # explainability
    fulltext_len = len(rep.full_text_snippet)
    explain = 3 if fulltext_len >= 1200 else (2 if fulltext_len >= 600 else 1)

    # listener_relevance
    zh_count = sum(1 for it in items if it.language == "zh")
    relevance_kws = ("å¤§æ¨¡å‹", "llm", "chatgpt", "claude", "gpt", "agent", "æ™ºèƒ½ä½“")
    has_relevance = any(k in text for k in relevance_kws)
    relevance = (
        3
        if (zh_count > 0 and has_relevance)
        else (2 if has_relevance or zh_count > 0 else 1)
    )

    # source_richness
    auth_scores = [_AUTHORITY_ORDER.get(it.source_category, 5) for it in items]
    min_auth = min(auth_scores) if auth_scores else 5
    richness = 3 if min_auth <= 1 else (2 if min_auth <= 2 else 1)

    return {
        "impact_scope": impact,
        "novelty": novelty,
        "explainability": explain,
        "listener_relevance": relevance,
        "source_richness": richness,
    }


# ---------------------------------------------------------------------------
# è§’è‰²åˆ†é… (PLAN Â§2.5)
# ---------------------------------------------------------------------------


def _assign_role(total: int, thresholds: dict[str, Any]) -> tuple[str, str]:
    """
    12-15 â†’ ğŸ”´main  |  8-11 â†’ ğŸŸ¡supporting  |  5-7 â†’ ğŸŸ¢quick  |  <5 â†’ âšªskip
    """
    main_range = thresholds.get("main", [12, 15])
    supporting_range = thresholds.get("supporting", [8, 11])
    quick_range = thresholds.get("quick", [5, 7])
    skip_below = thresholds.get("skip_below", 5)

    if total >= main_range[0]:
        return "main", "ğŸ”´"
    elif total >= supporting_range[0]:
        return "supporting", "ğŸŸ¡"
    elif total >= quick_range[0]:
        return "quick", "ğŸŸ¢"
    else:
        return "skip", "âšª"


# ---------------------------------------------------------------------------
# Thesis æç‚¼ (PLAN Â§2.6)
# ---------------------------------------------------------------------------

_THESIS_TEMPLATES = [
    "ä»Šå¤©çš„AIé¢†åŸŸï¼Œ{main_topic}æ­£åœ¨é‡å¡‘è¡Œä¸šæ ¼å±€",
    "ä»{main_topic}åˆ°{sub_topic}ï¼ŒAIæŠ€æœ¯æŒç»­åŠ é€Ÿæ¼”è¿›",
    "{main_topic}â€”â€”è¿™å¯èƒ½æ˜¯æœ¬å‘¨æœ€å€¼å¾—å…³æ³¨çš„AIè¶‹åŠ¿",
    "å½“{main_topic}é‡ä¸Šå®é™…è½åœ°ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä»€ä¹ˆ",
    "å›´ç»•{main_topic}ï¼Œå¤šæ–¹åŠ›é‡æ­£åœ¨æ±‡èš",
]


def _extract_thesis(scored_stories: list[ScoredStory]) -> str:
    """ä»ä¸»æ•…äº‹å’Œæ”¯æ’‘æ•…äº‹ä¸­æç‚¼ä¸€å¥ä¸»çº¿è®ºç‚¹ã€‚"""
    main_stories = [s for s in scored_stories if s.role == "main"]
    supporting = [s for s in scored_stories if s.role == "supporting"]

    if not main_stories:
        return "ä»Šå¤©çš„AIé¢†åŸŸåŠ¨æ€ä¸°å¯Œï¼Œå¤šä¸ªæ–¹å‘é½å¤´å¹¶è¿›"

    main_topic = main_stories[0].representative_title
    # ç®€åŒ–ï¼šå–å‰ 15 å­—ä½œä¸ºè¯é¢˜
    if len(main_topic) > 15:
        main_topic = main_topic[:15] + "..."

    sub_topic = ""
    if supporting:
        sub_topic = supporting[0].representative_title
        if len(sub_topic) > 12:
            sub_topic = sub_topic[:12] + "..."

    import random

    template = random.choice(_THESIS_TEMPLATES)
    return template.format(main_topic=main_topic, sub_topic=sub_topic or "äº§ä¸šåº”ç”¨")


# ---------------------------------------------------------------------------
# ä¸»å…¥å£ â€” åŠ å·¥ç®¡çº¿
# ---------------------------------------------------------------------------


def process(
    raw_items: list[RawItem],
    *,
    processing_cfg: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """
    å®Œæ•´åŠ å·¥ç®¡çº¿ã€‚è¿”å› episode_brief dictã€‚

    Parameters
    ----------
    raw_items : æ¥è‡ª fetcher çš„åŸå§‹æ¡ç›®
    processing_cfg : config.yaml ä¸­çš„ processing æ®µ
    """
    cfg = processing_cfg or {}
    dedup_cfg = cfg.get("dedup", {})
    cluster_cfg = cfg.get("clustering", {})
    scoring_cfg = cfg.get("scoring", {})
    authority_cfg = cfg.get("source_authority", _AUTHORITY_ORDER)

    # 1) ä¸‰å±‚å»é‡
    deduped = dedup_pipeline(
        raw_items,
        rapidfuzz_threshold=int(dedup_cfg.get("rapidfuzz_threshold", 92)),
        jieba_overlap=float(dedup_cfg.get("jieba_keyword_overlap", 0.35)),
        title_sim=int(dedup_cfg.get("title_sim_threshold", 85)),
        window_hours=int(dedup_cfg.get("dedup_window_hours", 48)),
    )

    if not deduped:
        logger.warning("No items after dedup")
        return {
            "stories": [],
            "thesis": "",
            "metadata": {"total_raw": len(raw_items), "total_deduped": 0},
        }

    # 2) èšç±»
    ngram = tuple(cluster_cfg.get("ngram_range", [2, 4]))
    clusters = cluster_stories(
        deduped,
        ngram_range=ngram,  # type: ignore[arg-type]
        eps=float(cluster_cfg.get("eps", 0.35)),
        min_samples=int(cluster_cfg.get("min_samples", 2)),
    )

    # 3) ä¸ºæ¯ä¸ª cluster æ„å»º context + æ‰“åˆ† + è§’è‰²åˆ†é…
    role_thresholds = scoring_cfg.get(
        "role_thresholds",
        {
            "main": [12, 15],
            "supporting": [8, 11],
            "quick": [5, 7],
            "skip_below": 5,
        },
    )

    scored_stories: list[ScoredStory] = []
    for cluster in clusters:
        context = _build_context(cluster, source_authority=authority_cfg)
        scores = _score_cluster(cluster)
        total = sum(scores.values())
        role, emoji = _assign_role(total, role_thresholds)

        scored_stories.append(
            ScoredStory(
                cluster_id=cluster.cluster_id,
                representative_title=cluster.representative.title,
                items=[item.to_dict() for item in cluster.items],
                context=context,
                scores=scores,
                total_score=total,
                role=role,
                role_emoji=emoji,
            )
        )

    # æŒ‰æ€»åˆ†é™åºæ’åˆ—
    scored_stories.sort(key=lambda x: x.total_score, reverse=True)

    # 4) Thesis æç‚¼
    thesis = _extract_thesis(scored_stories)

    # 5) ç»„è£… episode_brief
    brief: dict[str, Any] = {
        "thesis": thesis,
        "stories": [],
        "metadata": {
            "total_raw": len(raw_items),
            "total_deduped": len(deduped),
            "total_clusters": len(clusters),
            "generated_at": datetime.now(tz=timezone.utc).isoformat(),
        },
    }

    for ss in scored_stories:
        brief["stories"].append(
            {
                "cluster_id": ss.cluster_id,
                "representative_title": ss.representative_title,
                "role": ss.role,
                "role_emoji": ss.role_emoji,
                "total_score": ss.total_score,
                "scores": ss.scores,
                "context": {
                    "factual_summary": ss.context.factual_summary,
                    "historical_background": ss.context.historical_background,
                    "sources_ranked": ss.context.sources_ranked,
                },
                "items": ss.items,
            }
        )

    return brief


class _NumpySafeEncoder(json.JSONEncoder):
    """Handle numpy int/float types that stdlib json cannot serialize."""

    def default(self, o: Any) -> Any:
        try:
            import numpy as np

            if isinstance(o, (np.integer,)):
                return int(o)
            if isinstance(o, (np.floating,)):
                return float(o)
            if isinstance(o, np.ndarray):
                return o.tolist()
        except ImportError:
            pass
        return super().default(o)


def save_brief(brief: dict[str, Any], path: Path) -> None:
    """ä¿å­˜ episode_brief.jsonã€‚"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(brief, f, ensure_ascii=False, indent=2, cls=_NumpySafeEncoder)
        f.write("\n")
    logger.info("Saved episode_brief to %s", path)
